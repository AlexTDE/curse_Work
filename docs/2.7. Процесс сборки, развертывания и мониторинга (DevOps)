2.7. Процесс сборки, развертывания и мониторинга (DevOps)
2.7.1. Конфигурация CI/CD пайплайна
Система интегрирована с процессом непрерывной интеграции и развертывания на базе GitHub Actions, что обеспечивает автоматизацию тестирования, сборки и публикации обновлений. Пайплайн включает несколько последовательных этапов: на стадии lint выполняется статический анализ кода Python средствами flake8 и black для контроля соблюдения стандартов кодирования PEP 8; на стадии test запускаются модульные тесты Django (unittest/pytest) с измерением покрытия кода (coverage.py), а также интеграционные тесты API через DRF test client; на стадии build создаётся Docker-образ приложения с тегированием по версии и фиксацией хеша коммита; на стадии deploy образ публикуется в container registry (Docker Hub или GitHub Container Registry) и автоматически развертывается на целевом сервере через SSH-подключение или оркестратор Kubernetes. При обнаружении ошибок на любом этапе пайплайн прерывается, и разработчики получают уведомления в систему совместной работы (Slack, Telegram), что минимизирует время реакции на проблемы и предотвращает попадание дефектного кода в production.​

2.7.2. Контейнеризация приложения (Dockerfile)
Приложение упаковано в Docker-контейнер на основе официального образа Python 3.11-slim, что гарантирует воспроизводимость окружения на всех стадиях жизненного цикла — от локальной разработки до промышленной эксплуатации. Dockerfile следует многослойной структуре: базовый слой включает системные зависимости (libpq-dev для PostgreSQL, libopencv-dev для обработки изображений), промежуточный слой устанавливает Python-зависимости из requirements.txt с использованием pip и виртуального окружения, финальный слой копирует исходный код приложения и статические файлы. Применяется многоступенчатая сборка (multi-stage build), при которой тяжёлые инструменты компиляции используются только на этапе подготовки зависимостей и отбрасываются в финальном образе, что сокращает размер контейнера и уменьшает поверхность атаки. Сопутствующие сервисы (PostgreSQL, Redis, Celery Worker, Nginx) также контейнеризованы и объединены в единую среду посредством docker-compose.yml, описывающего сеть, тома для персистентного хранения данных и файлов, переменные окружения и порядок запуска контейнеров; это позволяет развернуть полнофункциональный экземпляр системы одной командой docker-compose up.​

2.7.3. Стратегия развертывания (Deployment Strategy)
Для минимизации простоев и рисков при обновлении системы применяется стратегия rolling update (последовательное обновление) в связке с health checks. При развёртывании новой версии приложения в Kubernetes-кластере (или через Docker Swarm) запускаются дополнительные экземпляры контейнеров с обновлённым образом, проверяется их работоспособность через HTTP health endpoint (/health или /api/healthcheck), возвращающий статус готовности приложения, после чего старые экземпляры постепенно выводятся из балансировки и останавливаются. В случае обнаружения ошибок в новых контейнерах (неуспешные health checks, критичные ошибки в логах) происходит автоматический откат (rollback) к предыдущей стабильной версии через перезапуск старых реплик. Для критичных релизов может применяться стратегия Blue-Green deployment: развёртывается полная копия окружения с новой версией (green), параллельно работающая с текущей версией (blue), после валидации нагрузка переключается на green через обновление конфигурации балансировщика (например, изменение upstream в Nginx или обновление Ingress-правил в Kubernetes), что позволяет мгновенно откатиться к blue в случае выявления проблем. Canary deployment (постепенное перенаправление малой доли трафика на новую версию) может быть реализован на уровне Ingress-контроллера или service mesh (Istio), но в рамках текущей конфигурации данная стратегия зарезервирована для будущих итераций системы.​

2.7.4. План мониторинга и логирования (Monitoring & Logging)
Мониторинг состояния системы и её компонентов обеспечивается средствами Prometheus для сбора метрик, Grafana для визуализации и Flower для контроля очереди Celery-задач. Prometheus собирает метрики производительности Django-приложения (количество запросов, задержки, частота ошибок 4xx/5xx) через экспортер django-prometheus, метрики инфраструктуры (CPU, RAM, дисковое пространство, сетевой трафик) через node_exporter, а также состояние PostgreSQL (количество соединений, длительность запросов, размер таблиц) и Redis (использование памяти, количество ключей, latency операций) через соответствующие экспортеры. Grafana подключена к Prometheus в качестве источника данных и предоставляет преднастроенные дашборды, отображающие ключевые показатели системы в реальном времени: загрузку серверов, статусы прогонов тестов, динамику выявления дефектов, очередь Celery-задач с индикаторами времени ожидания и количества активных воркеров. Flower интегрирован с Celery и предоставляет веб-интерфейс для мониторинга задач (queued, running, succeeded, failed), просмотра истории выполнения, трассировки ошибок и управления воркерами (перезапуск, отмена задач). Логирование реализовано на нескольких уровнях: Django записывает логи приложения (запросы, ошибки, действия пользователей) в stdout с возможностью перенаправления в файлы или систему централизованного логирования; Celery фиксирует логи выполнения задач, включая стек трейсы при ошибках обработки изображений; Nginx логирует HTTP-запросы в формате access.log и ошибки в error.log. Для централизованного сбора, индексации и анализа логов предусмотрена интеграция с ELK Stack (Elasticsearch, Logstash, Kibana): Logstash агрегирует логи из всех источников, обогащает их метаданными (timestamp, hostname, severity), отправляет в Elasticsearch для индексирования и полнотекстового поиска, а Kibana предоставляет интерфейс для визуализации, построения запросов и настройки алертов, срабатывающих при превышении пороговых значений ошибок или аномальных паттернов активности, что позволяет оперативно реагировать на инциденты и проводить постмортем-анализ.​
